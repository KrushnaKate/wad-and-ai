import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk import pos_tag
from collections import Counter

# Download required NLTK data (only first time)
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger_eng')

# Step 1: Input Text
text = input("Enter your text: ")

# Step 2: Tokenization
tokens = word_tokenize(text)
print("\nTokens:", tokens)

# Step 3: Removing Stop Words
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]
print("\nFiltered Tokens (without stopwords):", filtered_tokens)

# Step 4: Word Frequency Count
word_freq = Counter(filtered_tokens)
print("\nWord Frequency:")
for word, freq in word_freq.items():
    print(f"{word}: {freq}")

# Step 5: POS Tagging
pos_tags = pos_tag(filtered_tokens)
print("\nPOS Tags:")
for word, tag in pos_tags:
    print(f"{word} -> {tag}")
